{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fc4895",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import AutoModelForCausalLM\n",
    "import torch\n",
    "import torch.nn.functional\n",
    "from tqdm.auto import tqdm\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46d68295",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afe9c0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/LaBSE-en-ru were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.decoder.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "labse_name = 'cointegrated/LaBSE-en-ru'\n",
    "labse_model = AutoModel.from_pretrained(labse_name)\n",
    "labse_tokenizer = AutoTokenizer.from_pretrained(labse_name)\n",
    "if torch.cuda.is_available():\n",
    "    labse_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58fc00cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "mname = 'sberbank-ai/rugpt3small_based_on_gpt2'\n",
    "gpt_tokenizer = AutoTokenizer.from_pretrained(mname)\n",
    "gpt_model = AutoModelForCausalLM.from_pretrained(mname)\n",
    "if torch.cuda.is_available():\n",
    "    gpt_model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b39dafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labse(texts):\n",
    "    encoded_input = labse_tokenizer(\n",
    "        texts, padding=True, truncation=True, max_length=64, return_tensors='pt'\n",
    "    ).to(labse_model.device)\n",
    "    with torch.no_grad():\n",
    "        model_output = labse_model(**encoded_input)\n",
    "    embeddings = model_output.pooler_output\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings.cpu().numpy()\n",
    "\n",
    "\n",
    "def get_sims(df, batch_size=32):\n",
    "    sims = []\n",
    "    for i in range(0, df.shape[0], batch_size):\n",
    "        batch = df.iloc[i: i+batch_size]\n",
    "        e1 = encode_labse(batch.text1.tolist())\n",
    "        e2 = encode_labse(batch.text2.tolist())\n",
    "        sims.extend((e1 * e2).sum(axis=1))\n",
    "    return np.array(sims)\n",
    "\n",
    "\n",
    "def get_random_sims(df, batch_size=32, random_state=1):\n",
    "    df2 = pd.DataFrame({\n",
    "        'text1': df.text1.tolist(),\n",
    "        'text2': df.text2.sample(frac=1.0, random_state=random_state).tolist()\n",
    "    })\n",
    "    return get_sims(df2, batch_size=batch_size)\n",
    "\n",
    "\n",
    "def get_bleu(df):\n",
    "    return np.array([sentence_bleu([row.text1], row.text2) for i, row in df.iterrows()])\n",
    "\n",
    "\n",
    "def ngrams(word, n=3):\n",
    "    return [word[i: i+n] for i in range(len(word)-n+1)]\n",
    "\n",
    "\n",
    "def common_grams(text1, text2):\n",
    "    g1 = {g for w in text1.lower().split() for n in range(3, 7) for g in ngrams(f' {w} ', n=n)}\n",
    "    g2 = {g for w in text2.lower().split() for n in range(3, 7) for g in ngrams(f' {w} ', n=n)}\n",
    "    return len(g1.intersection(g2)) / len(g1.union(g2))\n",
    "\n",
    "\n",
    "def get_char_ngram_overlap(df):\n",
    "    return np.array([common_grams(row.text1, row.text2) for i, row in df.iterrows()])\n",
    "\n",
    "\n",
    "def calc_gpt2_ppl_corpus(test_sentences, aggregate=False, sep='\\n'):\n",
    "    \"\"\" Calculate average perplexity per token and number of tokens in each text.\"\"\"\n",
    "    lls = []\n",
    "    weights = []\n",
    "    for text in tqdm(test_sentences):\n",
    "        encodings = gpt_tokenizer(f'{sep}{text}{sep}', return_tensors='pt')\n",
    "        input_ids = encodings.input_ids.to(gpt_model.device)\n",
    "        target_ids = input_ids.clone()\n",
    "\n",
    "        w = max(0, len(input_ids[0]) - 1)\n",
    "        if w > 0:\n",
    "            with torch.no_grad():\n",
    "                outputs = gpt_model(input_ids, labels=target_ids)\n",
    "                log_likelihood = outputs[0]\n",
    "                ll = log_likelihood.item()\n",
    "        else:\n",
    "            ll = 0\n",
    "        lls.append(ll)\n",
    "        weights.append(w)\n",
    "    likelihoods, weights = np.array(lls), np.array(weights)\n",
    "    if aggregate:\n",
    "        return sum(likelihoods * weights) / sum(weights)\n",
    "    return likelihoods, weights\n",
    "\n",
    "\n",
    "def analyze_pairs(texts1, texts2):\n",
    "    df = pd.DataFrame({'text1': texts1, 'text2': texts2})\n",
    "    b1 = get_bleu(df)\n",
    "    b2 = get_bleu(pd.DataFrame({'text1': texts2, 'text2': texts1}))\n",
    "    p1, w1 = calc_gpt2_ppl_corpus(df.text1.tolist())\n",
    "    p2, w2 = calc_gpt2_ppl_corpus(df.text2.tolist())\n",
    "    return {\n",
    "        'sim': get_sims(df).mean(),\n",
    "        'sim_random': get_random_sims(df).mean(),\n",
    "        'bleu_1': b1.mean(),\n",
    "        'bleu_2': b2.mean(),\n",
    "        'bleu': (b1+b2).mean() / 2,\n",
    "        'char_ngram_overlap': get_char_ngram_overlap(df).mean(),\n",
    "        'perp_1': (p1 * w1).sum() / w1.sum(),\n",
    "        'perp_2': (p2 * w2).sum() / w2.sum(),\n",
    "        'perp_mean': (p1 * w1 + p2 * w2).sum() / (w1 + w1).sum(),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "beb869f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data\\\\train.csv', index_col=0)\n",
    "train_small = pd.read_csv('data\\\\train_small.csv', index_col=0)\n",
    "val = pd.read_csv('data\\\\eval.csv', index_col=0)\n",
    "test = pd.read_csv('data\\\\test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e4e83de",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, train_small, val, test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42284d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data[data['size'].isin(['small','medium','large'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27a2be11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "para_phraser    715772\n",
       "ru_xlsum         52010\n",
       "ru_adapt         41494\n",
       "gazeta           33894\n",
       "ru_simp           6432\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['type'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e6f0c08b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru_xlsum\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 52010/52010 [13:09<00:00, 65.89it/s] \n",
      "100%|██████████| 52010/52010 [06:27<00:00, 134.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru_adapt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 41494/41494 [05:12<00:00, 132.72it/s]\n",
      "100%|██████████| 41494/41494 [04:56<00:00, 140.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gazeta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 33894/33894 [14:05<00:00, 40.11it/s]\n",
      "100%|██████████| 33894/33894 [04:10<00:00, 135.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ru_simp\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 6432/6432 [00:46<00:00, 137.71it/s]\n",
      "100%|██████████| 6432/6432 [00:45<00:00, 141.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "para_phraser\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\nltk\\translate\\bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "100%|██████████| 715772/715772 [1:25:10<00:00, 140.06it/s]\n",
      "100%|██████████| 715772/715772 [1:26:26<00:00, 138.01it/s]\n"
     ]
    }
   ],
   "source": [
    "corpus_list = ['ru_xlsum', 'ru_adapt', 'gazeta', 'ru_simp','para_phraser']\n",
    "result_list = []\n",
    "for i in corpus_list:\n",
    "    subdata = data[data['type']==i]\n",
    "    result = analyze_pairs(subdata.source.values, subdata.target.values)\n",
    "    result['text_name'] = i\n",
    "    result_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52e6f2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = pd.DataFrame(result_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "79501064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sim</th>\n",
       "      <th>sim_random</th>\n",
       "      <th>bleu_1</th>\n",
       "      <th>bleu_2</th>\n",
       "      <th>bleu</th>\n",
       "      <th>char_ngram_overlap</th>\n",
       "      <th>perp_1</th>\n",
       "      <th>perp_2</th>\n",
       "      <th>perp_mean</th>\n",
       "      <th>text_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.537202</td>\n",
       "      <td>0.266430</td>\n",
       "      <td>0.010702</td>\n",
       "      <td>0.083270</td>\n",
       "      <td>0.046986</td>\n",
       "      <td>0.079309</td>\n",
       "      <td>3.031028</td>\n",
       "      <td>3.064053</td>\n",
       "      <td>1.648707</td>\n",
       "      <td>ru_xlsum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.742496</td>\n",
       "      <td>0.221048</td>\n",
       "      <td>0.528971</td>\n",
       "      <td>0.575525</td>\n",
       "      <td>0.552248</td>\n",
       "      <td>0.529097</td>\n",
       "      <td>4.057999</td>\n",
       "      <td>3.707350</td>\n",
       "      <td>3.067881</td>\n",
       "      <td>ru_adapt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.686615</td>\n",
       "      <td>0.259750</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.070985</td>\n",
       "      <td>0.035590</td>\n",
       "      <td>0.093601</td>\n",
       "      <td>2.986411</td>\n",
       "      <td>3.131570</td>\n",
       "      <td>1.619950</td>\n",
       "      <td>gazeta</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.771090</td>\n",
       "      <td>0.272943</td>\n",
       "      <td>0.365496</td>\n",
       "      <td>0.427363</td>\n",
       "      <td>0.396430</td>\n",
       "      <td>0.341063</td>\n",
       "      <td>4.236945</td>\n",
       "      <td>4.118633</td>\n",
       "      <td>3.335260</td>\n",
       "      <td>ru_simp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.662475</td>\n",
       "      <td>0.192331</td>\n",
       "      <td>0.320993</td>\n",
       "      <td>0.358986</td>\n",
       "      <td>0.339990</td>\n",
       "      <td>0.268770</td>\n",
       "      <td>4.650053</td>\n",
       "      <td>4.097087</td>\n",
       "      <td>3.529901</td>\n",
       "      <td>para_phraser</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sim  sim_random    bleu_1    bleu_2      bleu  char_ngram_overlap  \\\n",
       "0  0.537202    0.266430  0.010702  0.083270  0.046986            0.079309   \n",
       "1  0.742496    0.221048  0.528971  0.575525  0.552248            0.529097   \n",
       "2  0.686615    0.259750  0.000195  0.070985  0.035590            0.093601   \n",
       "3  0.771090    0.272943  0.365496  0.427363  0.396430            0.341063   \n",
       "4  0.662475    0.192331  0.320993  0.358986  0.339990            0.268770   \n",
       "\n",
       "     perp_1    perp_2  perp_mean     text_name  \n",
       "0  3.031028  3.064053   1.648707      ru_xlsum  \n",
       "1  4.057999  3.707350   3.067881      ru_adapt  \n",
       "2  2.986411  3.131570   1.619950        gazeta  \n",
       "3  4.236945  4.118633   3.335260       ru_simp  \n",
       "4  4.650053  4.097087   3.529901  para_phraser  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7538383b",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.to_csv('data_info.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
