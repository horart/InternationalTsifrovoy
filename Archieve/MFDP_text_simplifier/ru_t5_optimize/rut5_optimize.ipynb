{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40dac750",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import optuna\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "import mlflow\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "import rouge\n",
    "from ipynb.fs.full.sari import SARIsent\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "from tqdm.auto import trange\n",
    "import random\n",
    "import numpy as np\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pickle \n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import mlflow.tensorflow\n",
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from mlflow.models.signature import infer_signature\n",
    "from sklearn.utils import shuffle\n",
    "from torch import nn\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb0f3058",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data\\\\train_small_medium_mix_clean.csv', index_col=0)\n",
    "val = pd.read_csv('data\\\\eval.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73a970a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50157 2560\n"
     ]
    }
   ],
   "source": [
    "train_pairs = list(zip(train.source, train.target))\n",
    "\n",
    "val = val[val['size']=='small']\n",
    "eval_pairs = list(zip(val.source, val.target))\n",
    "\n",
    "print(train.shape[0], val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac00a608",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Params(object):\n",
    "    def __init__(self, batch_size, test_batch_size, epochs, lr, momentum, seed, cuda, log_interval):\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = test_batch_size\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.momentum = momentum\n",
    "        self.seed = seed\n",
    "        self.cuda = cuda\n",
    "        self.log_interval = log_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3080df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = \"rut5_optimize_clear_data\"\n",
    "mlflow.set_experiment(experiment_name)\n",
    "current_experiment = dict(mlflow.get_experiment_by_name(experiment_name))\n",
    "exp_id = current_experiment['experiment_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3233aec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Определение гиперпараметров\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 1e-4, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.3)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", 0, 1000)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 2, 12)\n",
    "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n",
    "    epochs = trial.suggest_int('epochs', 2, 5)\n",
    "    \n",
    "    batch_size = 3\n",
    "    #epochs = 2\n",
    "    \n",
    "    model_name = 'cointegrated/rut5-base-multitask'\n",
    "    model = T5ForConditionalGeneration.from_pretrained(model_name, num_layers=num_layers, dropout_rate=dropout_rate).cuda()\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    # Определение оптимизатора и планировщика\n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=len(train_pairs) * epochs)\n",
    "    \n",
    "    # Training loop\n",
    "    with mlflow.start_run(experiment_id=exp_id, run_name='first_test'):\n",
    "        mlflow.log_param(\"model_name\", 'rut5_multitask')\n",
    "        mlflow.log_param(\"batch_size\", learning_rate)\n",
    "        mlflow.log_param(\"epochs\", epochs)\n",
    "        mlflow.log_param(\"learning_rate\", batch_size)\n",
    "        mlflow.log_param(\"weight_decay\", weight_decay)\n",
    "        mlflow.log_param(\"warmup_steps\", warmup_steps)\n",
    "        mlflow.log_param(\"num_layers\", num_layers)\n",
    "        mlflow.log_param(\"dropout_rate\", dropout_rate)\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "\n",
    "            for i in trange(0, int(len(train_pairs) / batch_size)):\n",
    "                try:\n",
    "                    batch = train_pairs[i * batch_size: (i + 1) * batch_size]\n",
    "                    x = tokenizer([p[0] for p in batch], return_tensors='pt', padding=True).to(model.device)\n",
    "                    y = tokenizer([p[1] for p in batch], return_tensors='pt', padding=True).to(model.device)\n",
    "                    y.input_ids[y.input_ids == 0] = -100\n",
    "\n",
    "                except OutOfMemoryError:\n",
    "                    print('Ignoring batch due to CUDA out of memory')\n",
    "                    continue\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss = model(\n",
    "                    input_ids=x.input_ids,\n",
    "                    attention_mask=x.attention_mask,\n",
    "                    labels=y.input_ids,\n",
    "                    decoder_attention_mask=y.attention_mask,\n",
    "                    return_dict=True\n",
    "                ).loss\n",
    "\n",
    "                train_loss += loss.data.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "            train_loss /= int(len(train_pairs) / batch_size)\n",
    "            mlflow.log_metric(\"train_loss\", train_loss, epoch)\n",
    "            print(f'Epoch {epoch}, train_loss: {train_loss}')\n",
    "\n",
    "            # Validation loop\n",
    "            eval_loss = 0\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for j in trange(0, int(len(eval_pairs) / batch_size)):\n",
    "                    batch = eval_pairs[j * batch_size: (j + 1) * batch_size]\n",
    "                    x = tokenizer([p[0] for p in batch], return_tensors='pt', padding=True).to(model.device)\n",
    "                    y = tokenizer([p[1] for p in batch], return_tensors='pt', padding=True).to(model.device)\n",
    "\n",
    "                    loss = model(\n",
    "                        input_ids=x.input_ids,\n",
    "                        attention_mask=x.attention_mask,\n",
    "                        labels=y.input_ids,\n",
    "                        decoder_attention_mask=y.attention_mask,\n",
    "                        return_dict=True\n",
    "                    ).loss\n",
    "\n",
    "                    eval_loss += loss.data.item()\n",
    "\n",
    "            eval_loss /= int(len(eval_pairs) / batch_size)\n",
    "            mlflow.log_metric(\"eval_loss\", eval_loss, epoch)\n",
    "            print(f'Epoch {epoch}, eval_loss: {eval_loss}')\n",
    "        return eval_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb27fdca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-06-15 09:18:42,046]\u001b[0m A new study created in memory with name: no-name-7c8f1570-c737-4d80-a475-89d7a86606b0\u001b[0m\n",
      "Some weights of the model checkpoint at cointegrated/rut5-base-multitask were not used when initializing T5ForConditionalGeneration: ['encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.layer_norm.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "C:\\Users\\nkhozin\\anaconda3\\envs\\myenv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "100%|██████████| 16719/16719 [55:43<00:00,  5.00it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 13.910052919995307\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:12<00:00, 69.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, eval_loss: 9.769611346344316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16719/16719 [55:29<00:00,  5.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 7.823971821336457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:12<00:00, 69.17it/s]\n",
      "\u001b[32m[I 2023-06-15 11:10:21,428]\u001b[0m Trial 0 finished with value: 9.826596033670977 and parameters: {'learning_rate': 3.6605617340926676e-05, 'weight_decay': 0.01786410569573289, 'warmup_steps': 719, 'num_layers': 4, 'dropout_rate': 0.47430735054379025, 'epochs': 2}. Best is trial 0 with value: 9.826596033670977.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, eval_loss: 9.826596033670977\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rut5-base-multitask were not used when initializing T5ForConditionalGeneration: ['encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.3.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.3.layer.1.layer_norm.weight', 'encoder.block.6.layer.0.SelfAttention.v.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.4.layer.0.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.v.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.6.layer.0.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.0.layer_norm.weight', 'encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.3.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.2.layer.1.DenseReluDense.wo.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.2.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.q.weight', 'encoder.block.5.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.5.layer.0.layer_norm.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.5.layer.0.SelfAttention.q.weight', 'encoder.block.3.layer.0.SelfAttention.q.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.2.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.0.SelfAttention.k.weight', 'encoder.block.6.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.2.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.4.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.5.layer.0.SelfAttention.k.weight', 'encoder.block.2.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.5.layer.1.layer_norm.weight', 'encoder.block.4.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.6.layer.1.layer_norm.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.5.layer.0.SelfAttention.o.weight', 'encoder.block.3.layer.0.SelfAttention.v.weight', 'encoder.block.4.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.4.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.0.SelfAttention.k.weight', 'encoder.block.3.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.2.layer.0.layer_norm.weight', 'encoder.block.3.layer.0.layer_norm.weight', 'encoder.block.4.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.5.layer.0.SelfAttention.v.weight', 'encoder.block.6.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 16719/16719 [52:34<00:00,  5.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 19.92713874133197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:10<00:00, 78.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, eval_loss: 9.64582448603819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16719/16719 [52:34<00:00,  5.30it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 8.386021241509136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:10<00:00, 77.75it/s]\n",
      "\u001b[32m[I 2023-06-15 12:55:53,368]\u001b[0m Trial 1 finished with value: 9.644424479284432 and parameters: {'learning_rate': 1.4105405651541264e-05, 'weight_decay': 0.2954674567216341, 'warmup_steps': 973, 'num_layers': 2, 'dropout_rate': 0.47363517387136056, 'epochs': 2}. Best is trial 1 with value: 9.644424479284432.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, eval_loss: 9.644424479284432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rut5-base-multitask were not used when initializing T5ForConditionalGeneration: ['encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.0.layer_norm.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 16719/16719 [1:01:00<00:00,  4.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 13.42489545825452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:14<00:00, 58.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, eval_loss: 9.462740081974937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16719/16719 [1:01:02<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 7.768236127222859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:14<00:00, 59.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, eval_loss: 9.496355724781926\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16719/16719 [1:01:02<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss: 7.419482585538039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:13<00:00, 61.30it/s]\n",
      "\u001b[32m[I 2023-06-15 15:59:43,693]\u001b[0m Trial 2 finished with value: 9.327278432365318 and parameters: {'learning_rate': 1.9799206617982294e-05, 'weight_decay': 0.26707025078676655, 'warmup_steps': 444, 'num_layers': 7, 'dropout_rate': 0.4474496615923581, 'epochs': 3}. Best is trial 2 with value: 9.327278432365318.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, eval_loss: 9.327278432365318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cointegrated/rut5-base-multitask were not used when initializing T5ForConditionalGeneration: ['encoder.block.9.layer.1.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.0.layer_norm.weight', 'encoder.block.10.layer.1.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.layer_norm.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.0.SelfAttention.q.weight', 'encoder.block.8.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.k.weight', 'encoder.block.9.layer.0.SelfAttention.q.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.9.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.9.layer.0.SelfAttention.v.weight', 'encoder.block.7.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.10.layer.1.DenseReluDense.wo.weight', 'encoder.block.7.layer.1.DenseReluDense.wo.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.11.layer.1.DenseReluDense.wi_1.weight', 'encoder.block.11.layer.0.SelfAttention.o.weight', 'encoder.block.10.layer.0.SelfAttention.q.weight', 'encoder.block.10.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.v.weight', 'encoder.block.8.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.q.weight', 'encoder.block.11.layer.1.DenseReluDense.wo.weight', 'encoder.block.10.layer.0.layer_norm.weight', 'encoder.block.7.layer.0.SelfAttention.o.weight', 'encoder.block.11.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.0.SelfAttention.k.weight', 'encoder.block.7.layer.0.SelfAttention.v.weight', 'encoder.block.11.layer.1.layer_norm.weight', 'encoder.block.7.layer.1.layer_norm.weight', 'encoder.block.8.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.8.layer.1.layer_norm.weight', 'encoder.block.8.layer.0.SelfAttention.k.weight', 'encoder.block.8.layer.0.layer_norm.weight', 'encoder.block.9.layer.0.SelfAttention.o.weight', 'encoder.block.9.layer.1.DenseReluDense.wi_0.weight', 'encoder.block.7.layer.0.layer_norm.weight']\n",
      "- This IS expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing T5ForConditionalGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "100%|██████████| 16719/16719 [1:01:10<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, train_loss: 2.0155553501336647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:14<00:00, 59.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, eval_loss: 4.903908058738932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16719/16719 [1:01:10<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, train_loss: 1.6606139590205249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:14<00:00, 60.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, eval_loss: 6.461407265657557\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16719/16719 [1:01:08<00:00,  4.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, train_loss: 1.4947506007240134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 853/853 [00:14<00:00, 60.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, eval_loss: 6.779201609029021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 2505/16719 [09:30<51:38,  4.59it/s]  "
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(direction='minimize')\n",
    "\n",
    "# Запуск оптимизации гиперпараметров\n",
    "study.optimize(objective, n_trials=35)\n",
    "\n",
    "mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8999dd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Получение результатов\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(\"Best trial:\")\n",
    "print(\"Value: \", best_trial.value)\n",
    "print(\"Params: \")\n",
    "for key, value in best_trial.params.items():\n",
    "    print(\"{}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aedde58",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"optuna_trials_clean_data.obj\",\"wb\")\n",
    "pickle.dump(study.get_trials(), filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "daf926b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('optuna_trials.obj', 'rb') as f:\n",
    "    obj = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b20b9318",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "99f6b0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tb = []\n",
    "\n",
    "for i in range(len(obj)):\n",
    "    trial = obj[i]   \n",
    "    params = {}\n",
    "    params['iter'] = trial.number\n",
    "    params.update(trial.params)\n",
    "    params['loss'] = trial.values[0]\n",
    "    tb.append(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b87b284c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>iter</th>\n",
       "      <th>learning_rate</th>\n",
       "      <th>weight_decay</th>\n",
       "      <th>warmup_steps</th>\n",
       "      <th>num_layers</th>\n",
       "      <th>dropout_rate</th>\n",
       "      <th>loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>0.105954</td>\n",
       "      <td>177</td>\n",
       "      <td>7</td>\n",
       "      <td>0.338209</td>\n",
       "      <td>2.641003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.164190</td>\n",
       "      <td>523</td>\n",
       "      <td>4</td>\n",
       "      <td>0.126000</td>\n",
       "      <td>1.450552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.000037</td>\n",
       "      <td>0.077263</td>\n",
       "      <td>824</td>\n",
       "      <td>8</td>\n",
       "      <td>0.221739</td>\n",
       "      <td>1.581771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.000045</td>\n",
       "      <td>0.145387</td>\n",
       "      <td>908</td>\n",
       "      <td>11</td>\n",
       "      <td>0.100246</td>\n",
       "      <td>1.190233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.113149</td>\n",
       "      <td>118</td>\n",
       "      <td>10</td>\n",
       "      <td>0.222361</td>\n",
       "      <td>1.508529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.101248</td>\n",
       "      <td>477</td>\n",
       "      <td>6</td>\n",
       "      <td>0.144912</td>\n",
       "      <td>1.364144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000016</td>\n",
       "      <td>0.196126</td>\n",
       "      <td>386</td>\n",
       "      <td>12</td>\n",
       "      <td>0.484830</td>\n",
       "      <td>2.387404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>0.043701</td>\n",
       "      <td>367</td>\n",
       "      <td>9</td>\n",
       "      <td>0.225881</td>\n",
       "      <td>1.513977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.000036</td>\n",
       "      <td>0.146940</td>\n",
       "      <td>477</td>\n",
       "      <td>2</td>\n",
       "      <td>0.305286</td>\n",
       "      <td>6.200557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.000032</td>\n",
       "      <td>0.064082</td>\n",
       "      <td>730</td>\n",
       "      <td>10</td>\n",
       "      <td>0.287230</td>\n",
       "      <td>1.703320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>0.253162</td>\n",
       "      <td>980</td>\n",
       "      <td>12</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>1.218332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>0.273856</td>\n",
       "      <td>997</td>\n",
       "      <td>12</td>\n",
       "      <td>0.107814</td>\n",
       "      <td>1.256005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>0.238304</td>\n",
       "      <td>986</td>\n",
       "      <td>12</td>\n",
       "      <td>0.100298</td>\n",
       "      <td>1.186538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.224988</td>\n",
       "      <td>760</td>\n",
       "      <td>10</td>\n",
       "      <td>0.175786</td>\n",
       "      <td>1.385853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.000047</td>\n",
       "      <td>0.007241</td>\n",
       "      <td>866</td>\n",
       "      <td>11</td>\n",
       "      <td>0.168190</td>\n",
       "      <td>1.345884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>0.206657</td>\n",
       "      <td>668</td>\n",
       "      <td>8</td>\n",
       "      <td>0.102228</td>\n",
       "      <td>1.389988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.284096</td>\n",
       "      <td>886</td>\n",
       "      <td>5</td>\n",
       "      <td>0.186971</td>\n",
       "      <td>1.629849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.000048</td>\n",
       "      <td>0.239704</td>\n",
       "      <td>631</td>\n",
       "      <td>11</td>\n",
       "      <td>0.146143</td>\n",
       "      <td>1.280831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.175262</td>\n",
       "      <td>917</td>\n",
       "      <td>9</td>\n",
       "      <td>0.146978</td>\n",
       "      <td>1.311302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.000043</td>\n",
       "      <td>0.293165</td>\n",
       "      <td>775</td>\n",
       "      <td>11</td>\n",
       "      <td>0.199341</td>\n",
       "      <td>1.438110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.141095</td>\n",
       "      <td>590</td>\n",
       "      <td>2</td>\n",
       "      <td>0.103486</td>\n",
       "      <td>1.606097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.000071</td>\n",
       "      <td>0.250845</td>\n",
       "      <td>992</td>\n",
       "      <td>12</td>\n",
       "      <td>0.111616</td>\n",
       "      <td>1.224913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.000059</td>\n",
       "      <td>0.258192</td>\n",
       "      <td>939</td>\n",
       "      <td>12</td>\n",
       "      <td>0.145935</td>\n",
       "      <td>1.303198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.207702</td>\n",
       "      <td>820</td>\n",
       "      <td>11</td>\n",
       "      <td>0.100242</td>\n",
       "      <td>1.233055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.000060</td>\n",
       "      <td>0.230330</td>\n",
       "      <td>934</td>\n",
       "      <td>9</td>\n",
       "      <td>0.138822</td>\n",
       "      <td>1.292369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.000040</td>\n",
       "      <td>0.299073</td>\n",
       "      <td>996</td>\n",
       "      <td>12</td>\n",
       "      <td>0.159199</td>\n",
       "      <td>1.294719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.000077</td>\n",
       "      <td>0.263660</td>\n",
       "      <td>16</td>\n",
       "      <td>10</td>\n",
       "      <td>0.130737</td>\n",
       "      <td>1.454211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.183548</td>\n",
       "      <td>717</td>\n",
       "      <td>11</td>\n",
       "      <td>0.179530</td>\n",
       "      <td>1.386621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.000070</td>\n",
       "      <td>0.221213</td>\n",
       "      <td>843</td>\n",
       "      <td>8</td>\n",
       "      <td>0.129350</td>\n",
       "      <td>1.328412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.000089</td>\n",
       "      <td>0.250338</td>\n",
       "      <td>327</td>\n",
       "      <td>7</td>\n",
       "      <td>0.204734</td>\n",
       "      <td>2.193923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.000093</td>\n",
       "      <td>0.195577</td>\n",
       "      <td>249</td>\n",
       "      <td>12</td>\n",
       "      <td>0.255137</td>\n",
       "      <td>2.751597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.267991</td>\n",
       "      <td>998</td>\n",
       "      <td>12</td>\n",
       "      <td>0.119790</td>\n",
       "      <td>1.222308</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.000051</td>\n",
       "      <td>0.270179</td>\n",
       "      <td>926</td>\n",
       "      <td>11</td>\n",
       "      <td>0.126408</td>\n",
       "      <td>1.236329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.000062</td>\n",
       "      <td>0.240162</td>\n",
       "      <td>814</td>\n",
       "      <td>12</td>\n",
       "      <td>0.161995</td>\n",
       "      <td>1.339285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>0.284238</td>\n",
       "      <td>959</td>\n",
       "      <td>10</td>\n",
       "      <td>0.122239</td>\n",
       "      <td>1.240709</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    iter  learning_rate  weight_decay  warmup_steps  num_layers  dropout_rate  \\\n",
       "0      0       0.000098      0.105954           177           7      0.338209   \n",
       "1      1       0.000055      0.164190           523           4      0.126000   \n",
       "2      2       0.000037      0.077263           824           8      0.221739   \n",
       "3      3       0.000045      0.145387           908          11      0.100246   \n",
       "4      4       0.000025      0.113149           118          10      0.222361   \n",
       "5      5       0.000028      0.101248           477           6      0.144912   \n",
       "6      6       0.000016      0.196126           386          12      0.484830   \n",
       "7      7       0.000011      0.043701           367           9      0.225881   \n",
       "8      8       0.000036      0.146940           477           2      0.305286   \n",
       "9      9       0.000032      0.064082           730          10      0.287230   \n",
       "10    10       0.000066      0.253162           980          12      0.104167   \n",
       "11    11       0.000073      0.273856           997          12      0.107814   \n",
       "12    12       0.000057      0.238304           986          12      0.100298   \n",
       "13    13       0.000048      0.224988           760          10      0.175786   \n",
       "14    14       0.000047      0.007241           866          11      0.168190   \n",
       "15    15       0.000088      0.206657           668           8      0.102228   \n",
       "16    16       0.000068      0.284096           886           5      0.186971   \n",
       "17    17       0.000048      0.239704           631          11      0.146143   \n",
       "18    18       0.000023      0.175262           917           9      0.146978   \n",
       "19    19       0.000043      0.293165           775          11      0.199341   \n",
       "20    20       0.000059      0.141095           590           2      0.103486   \n",
       "21    21       0.000071      0.250845           992          12      0.111616   \n",
       "22    22       0.000059      0.258192           939          12      0.145935   \n",
       "23    23       0.000085      0.207702           820          11      0.100242   \n",
       "24    24       0.000060      0.230330           934           9      0.138822   \n",
       "25    25       0.000040      0.299073           996          12      0.159199   \n",
       "26    26       0.000077      0.263660            16          10      0.130737   \n",
       "27    27       0.000055      0.183548           717          11      0.179530   \n",
       "28    28       0.000070      0.221213           843           8      0.129350   \n",
       "29    29       0.000089      0.250338           327           7      0.204734   \n",
       "30    30       0.000093      0.195577           249          12      0.255137   \n",
       "31    31       0.000068      0.267991           998          12      0.119790   \n",
       "32    32       0.000051      0.270179           926          11      0.126408   \n",
       "33    33       0.000062      0.240162           814          12      0.161995   \n",
       "34    34       0.000053      0.284238           959          10      0.122239   \n",
       "\n",
       "        loss  \n",
       "0   2.641003  \n",
       "1   1.450552  \n",
       "2   1.581771  \n",
       "3   1.190233  \n",
       "4   1.508529  \n",
       "5   1.364144  \n",
       "6   2.387404  \n",
       "7   1.513977  \n",
       "8   6.200557  \n",
       "9   1.703320  \n",
       "10  1.218332  \n",
       "11  1.256005  \n",
       "12  1.186538  \n",
       "13  1.385853  \n",
       "14  1.345884  \n",
       "15  1.389988  \n",
       "16  1.629849  \n",
       "17  1.280831  \n",
       "18  1.311302  \n",
       "19  1.438110  \n",
       "20  1.606097  \n",
       "21  1.224913  \n",
       "22  1.303198  \n",
       "23  1.233055  \n",
       "24  1.292369  \n",
       "25  1.294719  \n",
       "26  1.454211  \n",
       "27  1.386621  \n",
       "28  1.328412  \n",
       "29  2.193923  \n",
       "30  2.751597  \n",
       "31  1.222308  \n",
       "32  1.236329  \n",
       "33  1.339285  \n",
       "34  1.240709  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(tb).drop('number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5929c85b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>number</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>learning_rate</th>\n",
       "      <td>0.000098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>weight_decay</th>\n",
       "      <td>0.105954</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>warmup_steps</th>\n",
       "      <td>177.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>num_layers</th>\n",
       "      <td>7.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dropout_rate</th>\n",
       "      <td>0.338209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loss</th>\n",
       "      <td>2.641003</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        0\n",
       "number           0.000000\n",
       "learning_rate    0.000098\n",
       "weight_decay     0.105954\n",
       "warmup_steps   177.000000\n",
       "num_layers       7.000000\n",
       "dropout_rate     0.338209\n",
       "loss             2.641003"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame.from_dict(params, orient='index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
